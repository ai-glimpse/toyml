{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ToyML: Machine Learning from Scratch","text":"<p>There are machine learning algorithms implemented from scratch. Let's learn machine learning with simple toy code.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install toyml\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li>Documentation: https://ai-glimpse.github.io/toyml/</li> <li>PyPi: https://pypi.org/project/toyml/</li> <li>Changelog: https://ai-glimpse.github.io/toyml/CHANGELOG/</li> </ul>"},{"location":"#roadmap","title":"RoadMap","text":"<ul> <li> Clustering: DBSCAN, Hierarchical(Agnes&amp;Diana), Kmeans</li> <li> Classification: KNN</li> <li> Ensemble: Boosting(AdaBoost)</li> <li> Classification: NaiveBayes, DecisionTree, SVM</li> <li> Association Analysis: Apriori</li> <li> Ensemble: GBDT</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>Refactored all the clustering algorithm</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Kmeans simple implementation fix centroid calculation</li> <li>Bisect K-means cluster with wrong dataset index</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Package management migrates from Poetry to Uv</li> </ul>"},{"location":"changelog/#020-2022-12-03","title":"[0.2.0] - 2022-12-03","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Clustering: DBSCAN, Hierarchical(Agnes&amp;Diana), Kmeans</li> <li>Classification: KNN</li> <li>Ensemble: Boosting(AdaBoost)</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Use Poetry to manage the package</li> <li>Use MkDocs to build documentation</li> </ul>"},{"location":"changelog/#010-2022-12-02","title":"[0.1.0] - 2022-12-02","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Project clean &amp; reorg</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"algorithms/classification/knn/","title":"KNN","text":""},{"location":"algorithms/classification/knn/#toyml.classification.knn.KNeighborsClassifier","title":"toyml.classification.knn.KNeighborsClassifier","text":"<pre><code>KNeighborsClassifier(\n    dataset: DataSet,\n    labels: Labels,\n    k: int,\n    dist: Callable[\n        [Vector, Vector], float\n    ] = euclidean_distance,\n    std: bool = True,\n)\n</code></pre> <p>The implementation of K-Nearest Neighbors.</p> <p>Ref: 1. Li Hang 2. Tan 3. Zhou 4. Murphy 5. Harrington</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def __init__(\n    self,\n    dataset: DataSet,\n    labels: Labels,\n    k: int,\n    dist: Callable[[Vector, Vector], float] = euclidean_distance,\n    std: bool = True,\n) -&gt; None:\n    if not isinstance(dataset, list):\n        raise TypeError(f\"invalid type in {type(dataset)} for the 'dataset' argument\")\n    self._dataset = dataset\n    self._labels = labels\n    self._k = k\n    self._dist = dist\n    # for standardization\n    self._is_std = std\n    self._means = vectors_mean(dataset)\n    self._stds = vectors_std(dataset)\n</code></pre>"},{"location":"algorithms/clustering/agnes/","title":"AGNES","text":""},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree","title":"toyml.clustering.agnes.ClusterTree  <code>dataclass</code>","text":"<pre><code>ClusterTree(\n    cluster_index: int,\n    parent: Optional[ClusterTree] = None,\n    children: list[ClusterTree] = list(),\n    sample_indices: list[int] = list(),\n    children_cluster_distance: Optional[float] = None,\n)\n</code></pre> <p>Represents a node in the hierarchical clustering tree.</p> <p>Each node is a cluster containing sample indices. Leaf nodes represent individual samples, while internal nodes represent merged clusters. The root node contains all samples.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.parent","title":"parent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parent: Optional[ClusterTree] = None\n</code></pre> <p>Parent node.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.children","title":"children  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>children: list[ClusterTree] = field(default_factory=list)\n</code></pre> <p>Children nodes.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.sample_indices","title":"sample_indices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_indices: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster: dataset sample indices.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES","title":"toyml.clustering.agnes.AGNES  <code>dataclass</code>","text":"<pre><code>AGNES(\n    n_cluster: int,\n    linkage: Literal[\n        \"single\", \"complete\", \"average\"\n    ] = \"single\",\n    distance_matrix_: list[list[float]] = list(),\n    clusters_: list[ClusterTree] = list(),\n    labels_: list[int] = list(),\n    cluster_tree_: Optional[ClusterTree] = None,\n    linkage_matrix: list[list[float]] = list(),\n    _cluster_index: int = 0,\n)\n</code></pre> <p>Agglomerative clustering algorithm (Bottom-up Hierarchical Clustering)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import AGNES\n&gt;&gt;&gt; dataset = [[1, 0], [1, 1], [1, 2], [10, 0], [10, 1], [10, 2]]\n&gt;&gt;&gt; agnes = AGNES(n_cluster=2).fit(dataset)\n&gt;&gt;&gt; print(agnes.labels_)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Using fit_predict method\n&gt;&gt;&gt; labels = agnes.fit_predict(dataset)\n&gt;&gt;&gt; print(labels)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Using different linkage methods\n&gt;&gt;&gt; agnes = AGNES(n_cluster=2, linkage=\"complete\").fit(dataset)\n&gt;&gt;&gt; print(agnes.labels_)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Plotting dendrogram\n&gt;&gt;&gt; agnes = AGNES(n_cluster=1).fit(dataset)\n&gt;&gt;&gt; agnes.plot_dendrogram(show=True)\n</code></pre> The AGNES Dendrogram Plot <p></p> References <ol> <li>Zhou Zhihua</li> <li>Tan</li> </ol>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.n_cluster","title":"n_cluster  <code>instance-attribute</code>","text":"<pre><code>n_cluster: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.linkage","title":"linkage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linkage: Literal[\"single\", \"complete\", \"average\"] = \"single\"\n</code></pre> <p>The linkage method to use.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.distance_matrix_","title":"distance_matrix_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_matrix_: list[list[float]] = field(\n    default_factory=list\n)\n</code></pre> <p>The distance matrix.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.clusters_","title":"clusters_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters_: list[ClusterTree] = field(default_factory=list)\n</code></pre> <p>The clusters.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The labels of each sample.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; AGNES\n</code></pre> <p>Fit the model.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; AGNES:\n    \"\"\"\n    Fit the model.\n    \"\"\"\n    self._validate(dataset)\n    n = len(dataset)\n    self.clusters_ = [ClusterTree(cluster_index=i, sample_indices=[i]) for i in range(n)]\n    self._cluster_index = n\n    self.distance_matrix_ = self._get_init_distance_matrix(dataset)\n    while len(self.clusters_) &gt; self.n_cluster:\n        (i, j), cluster_ij_distance = self._get_closest_clusters()\n        # merge cluster_i and cluster_j\n        self._merge_clusters(i, j, cluster_ij_distance)\n        # update distance matrix\n        self._update_distance_matrix(dataset, i, j)\n    # build cluster_tree_\n    self.cluster_tree_ = self._build_cluster_tree(n)\n    # assign dataset labels\n    self._get_labels(len(dataset))\n    return self\n</code></pre>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit the model and return the labels of each sample.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"\n    Fit the model and return the labels of each sample.\n    \"\"\"\n    self.fit(dataset)\n    return self.labels_\n</code></pre>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.plot_dendrogram","title":"plot_dendrogram","text":"<pre><code>plot_dendrogram(\n    figure_name: str = \"agnes_dendrogram.png\",\n    show: bool = False,\n) -&gt; None\n</code></pre> <p>Plot the dendrogram of the clustering result.</p> <p>This method visualizes the hierarchical structure of the clustering using a dendrogram. It requires the number of clusters to be set to 1 during initialization.</p> PARAMETER DESCRIPTION <code>figure_name</code> <p>The filename for saving the plot.                Defaults to \"agnes_dendrogram.png\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'agnes_dendrogram.png'</code> </p> <code>show</code> <p>If True, displays the plot. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the number of clusters is not 1.</p> Note <p>This method requires matplotlib and scipy to be installed.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def plot_dendrogram(\n    self,\n    figure_name: str = \"agnes_dendrogram.png\",\n    show: bool = False,\n) -&gt; None:\n    \"\"\"\n    Plot the dendrogram of the clustering result.\n\n    This method visualizes the hierarchical structure of the clustering\n    using a dendrogram. It requires the number of clusters to be set to 1\n    during initialization.\n\n    Args:\n        figure_name: The filename for saving the plot.\n                           Defaults to \"agnes_dendrogram.png\".\n        show: If True, displays the plot. Defaults to False.\n\n    Raises:\n        ValueError: If the number of clusters is not 1.\n\n    Note:\n        This method requires matplotlib and scipy to be installed.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    from scipy.cluster.hierarchy import dendrogram\n\n    if self.n_cluster != 1:\n        raise ValueError(\"The number of clusters should be 1 to plot dendrogram\")\n    # Plot the dendrogram\n    plt.figure(figsize=(10, 7))\n    dendrogram(np.array(self.linkage_matrix))\n    plt.title(\"AGNES Dendrogram\")\n    plt.xlabel(\"Sample Index\")\n    plt.ylabel(\"Distance\")\n    plt.savefig(f\"{figure_name}\", dpi=300, bbox_inches=\"tight\")\n    if show:\n        plt.show()\n</code></pre>"},{"location":"algorithms/clustering/dbscan/","title":"DBSCAN","text":""},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset","title":"toyml.clustering.dbscan.Dataset  <code>dataclass</code>","text":"<pre><code>Dataset(data: list[list[float]])\n</code></pre> <p>Dataset for DBSCAN</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> ATTRIBUTE DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> <code>n</code> <p>The number of data points.</p> <p> TYPE: <code>int</code> </p> <code>distance_matrix_</code> <p>The distance matrix.</p> <p> TYPE: <code>list[list[float]]</code> </p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset.get_neighbors","title":"get_neighbors","text":"<pre><code>get_neighbors(i: int, eps: float) -&gt; list[int]\n</code></pre> <p>Get the neighbors of the i-th data point.</p> PARAMETER DESCRIPTION <code>i</code> <p>The index of the data point.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>The indices of the neighbors(Don't include the point itself).</p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def get_neighbors(self, i: int, eps: float) -&gt; list[int]:\n    \"\"\"\n    Get the neighbors of the i-th data point.\n\n    Args:\n        i: The index of the data point.\n        eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n\n    Returns:\n        The indices of the neighbors(Don't include the point itself).\n    \"\"\"\n    return [j for j in range(self.n) if i != j and self.distance_matrix_[i][j] &lt;= eps]\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset.get_core_objects","title":"get_core_objects","text":"<pre><code>get_core_objects(\n    eps: float, min_samples: int\n) -&gt; tuple[set[int], list[int]]\n</code></pre> <p>Get the core objects and noises of the dataset.</p> PARAMETER DESCRIPTION <code>eps</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.</p> <p> TYPE: <code>float</code> </p> <code>min_samples</code> <p>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>core_objects</code> <p>The indices of the core objects.</p> <p> TYPE: <code>set[int]</code> </p> <code>noises</code> <p>The indices of the noises.</p> <p> TYPE: <code>list[int]</code> </p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def get_core_objects(self, eps: float, min_samples: int) -&gt; tuple[set[int], list[int]]:\n    \"\"\"\n    Get the core objects and noises of the dataset.\n\n    Args:\n        eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n        min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n\n    Returns:\n        core_objects: The indices of the core objects.\n        noises: The indices of the noises.\n    \"\"\"\n    core_objects = set()\n    noises = []\n    for i in range(self.n):\n        neighbors = self.get_neighbors(i, eps)\n        if len(neighbors) + 1 &gt;= min_samples:  # +1 to include the point itself\n            core_objects.add(i)\n        else:\n            noises.append(i)\n    return core_objects, noises\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN","title":"toyml.clustering.dbscan.DBSCAN  <code>dataclass</code>","text":"<pre><code>DBSCAN(\n    eps: float = 0.5,\n    min_samples: int = 5,\n    clusters_: list[list[int]] = list(),\n    core_objects_: set[int] = set(),\n    noises_: list[int] = list(),\n    labels_: list[int] = list(),\n)\n</code></pre> <p>DBSCAN algorithm</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import DBSCAN\n&gt;&gt;&gt; dataset = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n&gt;&gt;&gt; dbscan = DBSCAN(eps=3, min_samples=2).fit(dataset)\n&gt;&gt;&gt; dbscan.clusters_\n[[0, 1, 2], [3, 4]]\n&gt;&gt;&gt; dbscan.noises_\n[5]\n&gt;&gt;&gt; dbscan.labels_\n[0, 0, 0, 1, 1, -1]\n</code></pre> References <ol> <li>Zhou Zhihua</li> <li>Han</li> <li>Kassambara</li> <li>Wikipedia</li> </ol>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.eps","title":"eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eps: float = 0.5\n</code></pre> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function. (same as sklearn)</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.min_samples","title":"min_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_samples: int = 5\n</code></pre> <p>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. If min_samples is set to a higher value, DBSCAN will find denser clusters, whereas if it is set to a lower value, the found clusters will be more sparse. (same as sklearn)</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.clusters_","title":"clusters_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters_: list[list[int]] = field(default_factory=list)\n</code></pre> <p>The clusters found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.core_objects_","title":"core_objects_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>core_objects_: set[int] = field(default_factory=set)\n</code></pre> <p>The core objects found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.noises_","title":"noises_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>noises_: list[int] = field(default_factory=list)\n</code></pre> <p>The noises found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.fit","title":"fit","text":"<pre><code>fit(data: list[list[float]]) -&gt; 'DBSCAN'\n</code></pre> <p>Fit the DBSCAN model.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted DBSCAN model.</p> <p> TYPE: <code>'DBSCAN'</code> </p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def fit(self, data: list[list[float]]) -&gt; \"DBSCAN\":\n    \"\"\"\n    Fit the DBSCAN model.\n\n    Args:\n        data: The dataset.\n\n    Returns:\n        self: The fitted DBSCAN model.\n    \"\"\"\n    dataset = Dataset(data)\n\n    # initialize the unvisited set\n    unvisited = set(range(dataset.n))\n    # get core objects\n    self.core_objects_, self.noises_ = dataset.get_core_objects(self.eps, self.min_samples)\n\n    # core objects used for training\n    if len(self.core_objects_) == 0:\n        logger.warning(\"No core objects found, all data points are noise. Try to adjust the hyperparameters.\")\n        return self\n\n    # set of core objects: unordered\n    core_object_set = self.core_objects_.copy()\n    while core_object_set:\n        unvisited_old = unvisited.copy()\n        core_object = core_object_set.pop()\n        queue: deque[int] = deque()\n        queue.append(core_object)\n        unvisited.remove(core_object)\n\n        while queue:\n            q = queue.popleft()\n            neighbors = dataset.get_neighbors(q, self.eps)\n            if len(neighbors) + 1 &gt;= self.min_samples:\n                delta = set(neighbors) &amp; unvisited\n                for point in delta:\n                    queue.append(point)\n                    unvisited.remove(point)\n\n        cluster = unvisited_old - unvisited\n        self.clusters_.append(list(cluster))\n        core_object_set -= cluster\n\n    self.labels_ = [-1] * dataset.n  # -1 means noise\n    for i, cluster_index in enumerate(self.clusters_):\n        for j in cluster_index:\n            self.labels_[j] = i\n    return self\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(data: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit the DBSCAN model and return the cluster labels.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>The cluster labels.</p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def fit_predict(self, data: list[list[float]]) -&gt; list[int]:\n    \"\"\"\n    Fit the DBSCAN model and return the cluster labels.\n\n    Args:\n        data: The dataset.\n\n    Returns:\n        The cluster labels.\n    \"\"\"\n    return self.fit(data).labels_\n</code></pre>"},{"location":"algorithms/clustering/kmeans/","title":"Kmeans","text":""},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans","title":"toyml.clustering.kmeans.Kmeans  <code>dataclass</code>","text":"<pre><code>Kmeans(\n    k: int,\n    max_iter: int = 500,\n    tol: float = 1e-05,\n    centroids_init_method: Literal[\n        \"random\", \"kmeans++\"\n    ] = \"random\",\n    random_seed: Optional[int] = None,\n    iter_: int = 0,\n    clusters: dict[int, list[int]] = dict(),\n    centroids: dict[int, list[float]] = dict(),\n    labels: list[int] = list(),\n)\n</code></pre> <p>K-means algorithm (with Kmeans++ initialization as option).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import Kmeans\n&gt;&gt;&gt; dataset = [[1.0, 2.0], [1.0, 4.0], [1.0, 0.0], [10.0, 2.0], [10.0, 4.0], [11.0, 0.0]]\n&gt;&gt;&gt; kmeans = Kmeans(k=2, random_seed=42).fit(dataset)\n&gt;&gt;&gt; kmeans.clusters\n{0: [3, 4, 5], 1: [0, 1, 2]}\n&gt;&gt;&gt; kmeans.centroids\n{0: [10.333333333333334, 2.0], 1: [1.0, 2.0]}\n&gt;&gt;&gt; kmeans.labels\n[1, 1, 1, 0, 0, 0]\n&gt;&gt;&gt; kmeans.predict([0, 1])\n1\n&gt;&gt;&gt; kmeans.iter_\n2\n</code></pre> <p>There is a <code>fit_predict</code> method that can be used to fit and predict.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import Kmeans\n&gt;&gt;&gt; dataset = [[1, 0], [1, 1], [1, 2], [10, 0], [10, 1], [10, 2]]\n&gt;&gt;&gt; Kmeans(k=2, random_seed=42).fit_predict(dataset)\n[1, 1, 1, 0, 0, 0]\n</code></pre> References <ol> <li>Zhou Zhihua</li> <li>Murphy</li> </ol> Note <p>Here we just implement the naive K-means algorithm.</p> See Also <ul> <li>Bisecting K-means algorithm: toyml.clustering.bisect_kmeans</li> </ul>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.max_iter","title":"max_iter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_iter: int = 500\n</code></pre> <p>The number of iterations the algorithm will run for if it does not converge before that.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.tol","title":"tol  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tol: float = 1e-05\n</code></pre> <p>The tolerance for convergence.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.centroids_init_method","title":"centroids_init_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>centroids_init_method: Literal[\"random\", \"kmeans++\"] = (\n    \"random\"\n)\n</code></pre> <p>The method to initialize the centroids.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: Optional[int] = None\n</code></pre> <p>The random seed used to initialize the centroids.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.clusters","title":"clusters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters: dict[int, list[int]] = field(default_factory=dict)\n</code></pre> <p>The clusters of the dataset.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.centroids","title":"centroids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>centroids: dict[int, list[float]] = field(\n    default_factory=dict\n)\n</code></pre> <p>The centroids of the clusters.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.labels","title":"labels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels of the dataset.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; 'Kmeans'\n</code></pre> <p>Fit the dataset with K-means algorithm.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the set of data points for clustering</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>'Kmeans'</code> <p>self.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; \"Kmeans\":\n    \"\"\"\n    Fit the dataset with K-means algorithm.\n\n    Args:\n        dataset: the set of data points for clustering\n\n    Returns:\n        self.\n    \"\"\"\n    self.centroids = self._get_initial_centroids(dataset)\n    for _ in range(self.max_iter):\n        self.iter_ += 1\n        prev_centroids = self.centroids\n        self._iter_step(dataset)\n        if self._is_converged(prev_centroids):\n            break\n    return self\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit and predict the cluster label of the dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the set of data points for clustering</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>Cluster labels of the dataset samples.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"\n    Fit and predict the cluster label of the dataset.\n\n    Args:\n        dataset: the set of data points for clustering\n\n    Returns:\n        Cluster labels of the dataset samples.\n    \"\"\"\n    return self.fit(dataset).labels\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.predict","title":"predict","text":"<pre><code>predict(point: list[float]) -&gt; int\n</code></pre> <p>Predict the label of the point.</p> PARAMETER DESCRIPTION <code>point</code> <p>The data point to predict.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The label of the point.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def predict(self, point: list[float]) -&gt; int:\n    \"\"\"\n    Predict the label of the point.\n\n    Args:\n        point: The data point to predict.\n\n    Returns:\n        The label of the point.\n\n    \"\"\"\n    if len(self.centroids) == 0:\n        raise ValueError(\"The model is not fitted yet\")\n    return self._get_centroid_label(point, self.centroids)\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans","title":"toyml.clustering.bisect_kmeans.BisectingKmeans  <code>dataclass</code>","text":"<pre><code>BisectingKmeans(\n    k: int,\n    cluster_tree: ClusterTree = ClusterTree(),\n    labels: list[int] = list(),\n)\n</code></pre> <p>Bisecting K-means algorithm. Belong to Divisive hierarchical clustering (DIANA) algorithm.(top-down)</p> References <ol> <li>Harrington</li> <li>Tan</li> </ol> See Also <ul> <li>K-means algorithm: toyml.clustering.kmeans</li> </ul>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.cluster_tree","title":"cluster_tree  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cluster_tree: ClusterTree = field(\n    default_factory=ClusterTree\n)\n</code></pre> <p>The cluster tree</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.labels","title":"labels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels of the dataset.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; 'BisectingKmeans'\n</code></pre> <p>Fit the dataset with Bisecting K-means algorithm.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The set of data points for clustering.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>'BisectingKmeans'</code> <p>self.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; \"BisectingKmeans\":\n    \"\"\"\n    Fit the dataset with Bisecting K-means algorithm.\n\n    Args:\n        dataset: The set of data points for clustering.\n\n    Returns:\n        self.\n\n    \"\"\"\n    n = len(dataset)\n    # check dataset\n    if self.k &gt; n:\n        raise ValueError(\n            f\"Number of clusters(k) cannot be greater than the number of samples(n), not get {self.k=} &gt; {n=}\"\n        )\n    # start with only one cluster which contains all the data points in dataset\n    cluster = list(range(n))\n    self.cluster_tree.cluster = cluster\n    self.cluster_tree.centroid = self._get_cluster_centroids(dataset, cluster)\n    self.labels = self._get_dataset_labels(dataset)\n    total_error = sum_square_error([dataset[i] for i in cluster])\n    # iterate until got k clusters\n    while len(self.cluster_tree.get_clusters()) &lt; self.k:\n        # init values for later iteration\n        to_splot_cluster_node = None\n        split_cluster_into: Optional[tuple[list[int], list[int]]] = None\n        for cluster_index, cluster_node in enumerate(self.cluster_tree.leaf_cluster_nodes()):\n            # perform K-means with k=2\n            cluster_data = [dataset[i] for i in cluster_node.cluster]\n            # If the cluster cannot be split further, skip it\n            if len(cluster_data) &lt; 2:\n                continue\n            # Bisect by kmeans with k=2\n            cluster_unsplit_error, cluster_split_error, (cluster1, cluster2) = self._bisect_by_kmeans(\n                cluster_data, cluster_node, dataset\n            )\n            new_total_error = total_error - cluster_unsplit_error + cluster_split_error\n            if new_total_error &lt; total_error:\n                total_error = new_total_error\n                split_cluster_into = (cluster1, cluster2)\n                to_splot_cluster_node = cluster_node\n\n        if to_splot_cluster_node is not None and split_cluster_into is not None:\n            self._commit_split(to_splot_cluster_node, split_cluster_into, dataset)\n            self.labels = self._get_dataset_labels(dataset)\n        else:\n            break\n\n    return self\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit and predict the cluster label of the dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The set of data points for clustering.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>Cluster labels of the dataset samples.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"\n    Fit and predict the cluster label of the dataset.\n\n    Args:\n        dataset: The set of data points for clustering.\n\n    Returns:\n        Cluster labels of the dataset samples.\n    \"\"\"\n    return self.fit(dataset).labels\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.bisect_kmeans.BisectingKmeans.predict","title":"predict","text":"<pre><code>predict(points: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Predict the cluster label of the given points.</p> PARAMETER DESCRIPTION <code>points</code> <p>A list of data points to predict.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>A list of predicted cluster labels for the input points.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model has not been fitted yet.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def predict(self, points: list[list[float]]) -&gt; list[int]:\n    \"\"\"\n    Predict the cluster label of the given points.\n\n    Args:\n        points: A list of data points to predict.\n\n    Returns:\n        A list of predicted cluster labels for the input points.\n\n    Raises:\n        ValueError: If the model has not been fitted yet.\n    \"\"\"\n    if self.cluster_tree.centroid is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n\n    clusters = self.cluster_tree.get_clusters()\n    predictions = []\n    for point in points:\n        node = self.cluster_tree\n        while not node.is_leaf():\n            if node.left is None or node.right is None:\n                raise ValueError(\"Invalid cluster tree structure.\")\n\n            dist_left = euclidean_distance(point, node.left.centroid)  # type: ignore[arg-type]\n            dist_right = euclidean_distance(point, node.right.centroid)  # type: ignore[arg-type]\n\n            node = node.left if dist_left &lt; dist_right else node.right\n        cluster_index = clusters.index(node.cluster)\n        predictions.append(cluster_index)\n\n    return predictions\n</code></pre>"},{"location":"algorithms/evt/spot/","title":"SPOT","text":"<p>SPOT<sup>1</sup> algorithm is backed by EVT.</p> <ol> <li> <p>A. Siffer, P.-A. Fouque, A. Termier, and C. Largouet, \u201cAnomaly detection in streams with extreme value theory,\u201d in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 1067\u20131075.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/1-read-first/","title":"Read First","text":""}]}