{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ToyML: Machine Learning from Scratch","text":"<p>There are machine learning algorithms implemented from scratch. Let's learn machine learning with simple toy code.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install toyml\n</code></pre>"},{"location":"#changelog","title":"Changelog","text":"<p>https://ai-glimpse.github.io/toyml/changelog/</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#040-2024-10-29","title":"[0.4.0] - 2024-10-29","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add isolation forest algorithm #130</li> </ul>"},{"location":"changelog/#032-2024-10-25","title":"[0.3.2] - 2024-10-25","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Add support for Python3.13</li> </ul>"},{"location":"changelog/#031-2024-09-19","title":"[0.3.1] - 2024-09-19","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improve the one dimension classifier implementation in adaboost algorithm #117</li> </ul>"},{"location":"changelog/#031rc0-2024-09-19","title":"[0.3.1.rc0] - 2024-09-19","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fix the codecov report issue in ci #112</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Improve the variable names in kmeans algorithm #114</li> <li>Improve the documentation and linters #115</li> </ul>"},{"location":"changelog/#030-2024-09-12","title":"[0.3.0] - 2024-09-12","text":""},{"location":"changelog/#docs","title":"Docs","text":"<ul> <li>Improve the documentation</li> </ul>"},{"location":"changelog/#030rc0-2024-09-12","title":"[0.3.0.rc0] - 2024-09-12","text":""},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>Refactored knn and adaboost algorithm</li> <li>Make all the algorithms in single file (almost, except for kmeans++)</li> </ul>"},{"location":"changelog/#030dev1-2024-09-06","title":"[0.3.0.dev1] - 2024-09-06","text":""},{"location":"changelog/#refactor_1","title":"Refactor","text":"<ul> <li>Refactored all the clustering algorithm</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Kmeans simple implementation fix centroid calculation</li> <li>Bisect K-means cluster with wrong dataset index</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Package management migrates from Poetry to Uv</li> </ul>"},{"location":"changelog/#020-2022-12-03","title":"[0.2.0] - 2022-12-03","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Clustering: DBSCAN, Hierarchical(Agnes&amp;Diana), Kmeans</li> <li>Classification: KNN</li> <li>Ensemble: Boosting(AdaBoost)</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Use Poetry to manage the package</li> <li>Use MkDocs to build documentation</li> </ul>"},{"location":"changelog/#010-2022-12-02","title":"[0.1.0] - 2022-12-02","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Project clean &amp; reorg</li> </ul>"},{"location":"algorithms/classification/knn/","title":"KNN","text":""},{"location":"algorithms/classification/knn/#toyml.classification.knn.KNN","title":"toyml.classification.knn.KNN  <code>dataclass</code>","text":"<pre><code>KNN(k: int, std_transform: bool = True, dataset_: list[list[float]] | None = None, labels_: list[Any] | None = None, standardizationer_: Standardizationer | None = None)\n</code></pre> <p>K-Nearest Neighbors classification algorithm implementation.</p> <p>This class implements the K-Nearest Neighbors algorithm for classification tasks. It supports optional standardization of the input data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]\n&gt;&gt;&gt; labels = [\"A\", \"A\", \"B\", \"B\"]\n&gt;&gt;&gt; knn = KNN(k=3, std_transform=True).fit(dataset, labels)\n&gt;&gt;&gt; knn.predict([2.5, 3.5])\n'A'\n</code></pre> ATTRIBUTE DESCRIPTION <code>k</code> <p>The number of nearest neighbors to consider for classification.</p> <p> TYPE: <code>int</code> </p> <code>std_transform</code> <p>Whether to standardize the input data (default: True).</p> <p> TYPE: <code>bool</code> </p> <code>dataset_</code> <p>The fitted dataset (standardized if std_transform is True).</p> <p> TYPE: <code>list[list[float]] | None</code> </p> <code>labels_</code> <p>The labels corresponding to the fitted dataset.</p> <p> TYPE: <code>list[Any] | None</code> </p> <code>standardizationer_</code> <p>The Standardizationer instance if std_transform is True.</p> <p> TYPE: <code>Standardizationer | None</code> </p> References <ol> <li>Li Hang</li> <li>Tan</li> <li>Zhou Zhihua</li> <li>Murphy</li> <li>Harrington</li> </ol>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.KNN.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]], labels: list[Any]) -&gt; KNN\n</code></pre> <p>Fit the KNN model to the given dataset and labels.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The input dataset to fit the model to.</p> <p> TYPE: <code>list[list[float]]</code> </p> <code>labels</code> <p>The labels corresponding to the input dataset.</p> <p> TYPE: <code>list[Any]</code> </p> RETURNS DESCRIPTION <code>KNN</code> <p>The fitted KNN instance.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def fit(self, dataset: list[list[float]], labels: list[Any]) -&gt; KNN:\n    \"\"\"Fit the KNN model to the given dataset and labels.\n\n    Args:\n        dataset: The input dataset to fit the model to.\n        labels: The labels corresponding to the input dataset.\n\n    Returns:\n        The fitted KNN instance.\n    \"\"\"\n    self.dataset_ = dataset\n    self.labels_ = labels\n    if self.std_transform:\n        self.standardizationer_ = Standardizationer()\n        self.dataset_ = self.standardizationer_.fit_transform(self.dataset_)\n    return self\n</code></pre>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.KNN.predict","title":"predict","text":"<pre><code>predict(x: list[float]) -&gt; Any\n</code></pre> <p>Predict the label of the input data.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input data to predict.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The predicted label.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model is not fitted yet.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def predict(self, x: list[float]) -&gt; Any:  # noqa: ANN401\n    \"\"\"Predict the label of the input data.\n\n    Args:\n        x: The input data to predict.\n\n    Returns:\n        The predicted label.\n\n    Raises:\n        ValueError: If the model is not fitted yet.\n    \"\"\"\n    if self.dataset_ is None or self.labels_ is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n\n    if self.std_transform:\n        if self.standardizationer_ is None:\n            msg = \"Cannot find the standardization!\"\n            raise ValueError(msg)\n        x = self.standardizationer_.transform([x])[0]\n    distances = [self._calculate_distance(x, point) for point in self.dataset_]\n    # get k-nearest neighbors' label\n    k_nearest_labels = [\n        label for _, label in sorted(zip(distances, self.labels_, strict=False), key=lambda x: x[0])\n    ][:: self.k]\n    label = Counter(k_nearest_labels).most_common(1)[0][0]\n    return label\n</code></pre>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.Standardizationer","title":"toyml.classification.knn.Standardizationer  <code>dataclass</code>","text":"<pre><code>Standardizationer(_means: list[float] = list(), _stds: list[float] = list(), _dimension: int | None = None)\n</code></pre> <p>A class for standardizing numerical datasets.</p> <p>Provides methods to fit a standardization model to a dataset, transform datasets using the fitted model, and perform both operations in a single step.</p>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.Standardizationer.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; Standardizationer\n</code></pre> <p>Fit the standardization model to the given dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The input dataset to fit the model to.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>Standardizationer</code> <p>The fitted Standardizationer instance.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the dataset has inconsistent dimensions.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; Standardizationer:\n    \"\"\"Fit the standardization model to the given dataset.\n\n    Args:\n        dataset: The input dataset to fit the model to.\n\n    Returns:\n        The fitted Standardizationer instance.\n\n    Raises:\n        ValueError: If the dataset has inconsistent dimensions.\n    \"\"\"\n    self._dimension = self._get_dataset_dimension(dataset)\n    self._means = self._dataset_column_means(dataset)\n    self._stds = self._dataset_column_stds(dataset)\n    return self\n</code></pre>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.Standardizationer.transform","title":"transform","text":"<pre><code>transform(dataset: list[list[float]]) -&gt; list[list[float]]\n</code></pre> <p>Transform the given dataset using the fitted standardization model.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The input dataset to transform.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[list[float]]</code> <p>The standardized dataset.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model has not been fitted yet.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def transform(self, dataset: list[list[float]]) -&gt; list[list[float]]:\n    \"\"\"Transform the given dataset using the fitted standardization model.\n\n    Args:\n        dataset: The input dataset to transform.\n\n    Returns:\n        The standardized dataset.\n\n    Raises:\n        ValueError: If the model has not been fitted yet.\n    \"\"\"\n    if self._dimension is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n    return self.standardization(dataset)\n</code></pre>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.Standardizationer.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(dataset: list[list[float]]) -&gt; list[list[float]]\n</code></pre> <p>Fit the standardization model to the dataset and transform it in one step.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The input dataset to fit and transform.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[list[float]]</code> <p>The standardized dataset.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def fit_transform(self, dataset: list[list[float]]) -&gt; list[list[float]]:\n    \"\"\"Fit the standardization model to the dataset and transform it in one step.\n\n    Args:\n        dataset: The input dataset to fit and transform.\n\n    Returns:\n        The standardized dataset.\n    \"\"\"\n    self.fit(dataset)\n    return self.transform(dataset)\n</code></pre>"},{"location":"algorithms/classification/knn/#toyml.classification.knn.Standardizationer.standardization","title":"standardization","text":"<pre><code>standardization(dataset: list[list[float]]) -&gt; list[list[float]]\n</code></pre> <p>Standardize the given numerical dataset.</p> <p>The standardization is performed by subtracting the mean and dividing by the standard deviation for each feature. When the standard deviation is 0, all the values in the column are the same, here we set std to 1 to make every value in the column become 0 and avoid division by zero.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The input dataset to standardize.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[list[float]]</code> <p>The standardized dataset.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model has not been fitted yet.</p> Source code in <code>toyml/classification/knn.py</code> <pre><code>def standardization(self, dataset: list[list[float]]) -&gt; list[list[float]]:\n    \"\"\"Standardize the given numerical dataset.\n\n    The standardization is performed by subtracting the mean and dividing\n    by the standard deviation for each feature.\n    When the standard deviation is 0, all the values in the column are the same,\n    here we set std to 1 to make every value in the column become 0 and avoid division by zero.\n\n    Args:\n        dataset: The input dataset to standardize.\n\n    Returns:\n        The standardized dataset.\n\n    Raises:\n        ValueError: If the model has not been fitted yet.\n    \"\"\"\n    if self._dimension is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n    for j, column in enumerate(zip(*dataset, strict=False)):\n        mean, std = self._means[j], self._stds[j]\n        # ref: https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/preprocessing/data.py#L70\n        if math.isclose(std, 0, abs_tol=1e-9):\n            std = 1\n        for i, value in enumerate(column):\n            dataset[i][j] = (value - mean) / std\n    return dataset\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/","title":"Naive Bayes","text":""},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.BaseNaiveBayes","title":"toyml.classification.naive_bayes.BaseNaiveBayes  <code>dataclass</code>","text":"<pre><code>BaseNaiveBayes(class_prior_: dict[Class, float] = dict())\n</code></pre> <p>               Bases: <code>ABC</code></p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.BaseNaiveBayes.class_prior_","title":"class_prior_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_prior_: dict[Class, float] = field(default_factory=dict)\n</code></pre> <p>The prior probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.BaseNaiveBayes.predict","title":"predict","text":"<pre><code>predict(sample: list[FeatureValue]) -&gt; int\n</code></pre> <p>Predict the class label for a given sample.</p> PARAMETER DESCRIPTION <code>sample</code> <p>A single sample to predict, represented as a list of feature values.</p> <p> TYPE: <code>list[FeatureValue]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Predicted class label.</p> <p> TYPE: <code>int</code> </p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def predict(self, sample: list[FeatureValue]) -&gt; int:\n    \"\"\"Predict the class label for a given sample.\n\n    Args:\n        sample: A single sample to predict, represented as a list of feature values.\n\n    Returns:\n        int: Predicted class label.\n    \"\"\"\n    label_posteriors = self.predict_proba(sample)\n    label = max(label_posteriors, key=lambda k: label_posteriors[k])\n    return label\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.BaseNaiveBayes.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(sample: list[FeatureValue], normalization: bool = True) -&gt; dict[Class, float]\n</code></pre> <p>Predict class probabilities for a given sample.</p> PARAMETER DESCRIPTION <code>sample</code> <p>A single sample to predict, represented as a list of feature values.</p> <p> TYPE: <code>list[FeatureValue]</code> </p> <code>normalization</code> <p>Whether to normalize the probabilities. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict[Class, float]</code> <p>dict[int, float]: Dictionary mapping class labels to their predicted probabilities.</p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def predict_proba(self, sample: list[FeatureValue], normalization: bool = True) -&gt; dict[Class, float]:\n    \"\"\"Predict class probabilities for a given sample.\n\n    Args:\n        sample: A single sample to predict, represented as a list of feature values.\n        normalization: Whether to normalize the probabilities. Default is True.\n\n    Returns:\n        dict[int, float]: Dictionary mapping class labels to their predicted probabilities.\n    \"\"\"\n    label_posteriors = self.predict_log_proba(sample, normalization)\n    return {label: math.exp(log_prob) for label, log_prob in label_posteriors.items()}\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.BaseNaiveBayes.predict_log_proba","title":"predict_log_proba","text":"<pre><code>predict_log_proba(sample: list[FeatureValue], normalization: bool = True) -&gt; dict[Class, float]\n</code></pre> <p>Predict log probabilities for a given sample.</p> PARAMETER DESCRIPTION <code>sample</code> <p>A single sample to predict, represented as a list of feature values.</p> <p> TYPE: <code>list[FeatureValue]</code> </p> <code>normalization</code> <p>Whether to normalize the log probabilities. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict[Class, float]</code> <p>dict[int, float]: Dictionary mapping class labels to their predicted log probabilities.</p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def predict_log_proba(self, sample: list[FeatureValue], normalization: bool = True) -&gt; dict[Class, float]:\n    \"\"\"Predict log probabilities for a given sample.\n\n    Args:\n        sample: A single sample to predict, represented as a list of feature values.\n        normalization: Whether to normalize the log probabilities. Default is True.\n\n    Returns:\n        dict[int, float]: Dictionary mapping class labels to their predicted log probabilities.\n    \"\"\"\n    label_likelihoods = self._log_likelihood(sample)\n    raw_label_posteriors: dict[int, float] = {}\n    for label, likelihood in label_likelihoods.items():\n        raw_label_posteriors[label] = likelihood + math.log(self.class_prior_[label])\n    if normalization is False:\n        return raw_label_posteriors\n    # ref: https://github.com/scikit-learn/scikit-learn/blob/2beed55847ee70d363bdbfe14ee4401438fba057/sklearn/naive_bayes.py#L97\n    max_log_prob = max(raw_label_posteriors.values())\n    logsumexp_prob = max_log_prob + math.log(\n        sum(math.exp(log_prob - max_log_prob) for log_prob in raw_label_posteriors.values()),\n    )\n    label_posteriors = {\n        label: raw_posterior - logsumexp_prob for label, raw_posterior in raw_label_posteriors.items()\n    }\n    return label_posteriors\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes","title":"toyml.classification.naive_bayes.GaussianNaiveBayes  <code>dataclass</code>","text":"<pre><code>GaussianNaiveBayes(class_prior_: dict[Class, float] = dict(), unbiased_variance: bool = True, var_smoothing: float = 1e-09, labels_: list[Class] = list(), class_count_: int = 0, means_: dict[Class, list[float]] = dict(), variances_: dict[Class, list[float]] = dict(), epsilon_: float = 0)\n</code></pre> <p>               Bases: <code>BaseNaiveBayes</code></p> <p>Gaussian naive bayes classification algorithm implementation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; label = [0, 0, 0, 0, 1, 1, 1, 1]\n&gt;&gt;&gt; dataset = [\n...     [6.00, 180, 12],\n...     [5.92, 190, 11],\n...     [5.58, 170, 12],\n...     [5.92, 165, 10],\n...     [5.00, 100, 6],\n...     [5.50, 150, 8],\n...     [5.42, 130, 7],\n...     [5.75, 150, 9],\n... ]\n&gt;&gt;&gt; clf = GaussianNaiveBayes().fit(dataset, label)\n&gt;&gt;&gt; clf.predict([6.00, 130, 8])\n1\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.unbiased_variance","title":"unbiased_variance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>unbiased_variance: bool = True\n</code></pre> <p>Use the unbiased variance estimation or not. Default is True.</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.var_smoothing","title":"var_smoothing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>var_smoothing: float = 1e-09\n</code></pre> <p>Portion of the largest variance of all features that is added to variances for calculation stability.</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[Class] = field(default_factory=list)\n</code></pre> <p>The labels in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.class_count_","title":"class_count_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_count_: int = 0\n</code></pre> <p>The number of classes in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.class_prior_","title":"class_prior_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_prior_: dict[Class, float] = field(default_factory=dict)\n</code></pre> <p>The prior probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.means_","title":"means_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>means_: dict[Class, list[float]] = field(default_factory=dict)\n</code></pre> <p>The means of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.variances_","title":"variances_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variances_: dict[Class, list[float]] = field(default_factory=dict)\n</code></pre> <p>The variance of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.epsilon_","title":"epsilon_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epsilon_: float = 0\n</code></pre> <p>The absolute additive value to variances.</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.GaussianNaiveBayes.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; GaussianNaiveBayes\n</code></pre> <p>Fit the Gaussian Naive Bayes classifier.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>Training data, where each row is a sample and each column is a feature.</p> <p> TYPE: <code>list[list[FeatureValue]]</code> </p> <code>labels</code> <p>Target labels for training data.</p> <p> TYPE: <code>list[Class]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>Returns the instance itself.</p> <p> TYPE: <code>GaussianNaiveBayes</code> </p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def fit(self, dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; GaussianNaiveBayes:\n    \"\"\"Fit the Gaussian Naive Bayes classifier.\n\n    Args:\n        dataset: Training data, where each row is a sample and each column is a feature.\n        labels: Target labels for training data.\n\n    Returns:\n        self: Returns the instance itself.\n    \"\"\"\n    self.labels_ = sorted(set(labels))\n    self.class_count_ = len(set(labels))\n    self.class_prior_ = {label: 1 / self.class_count_ for label in self.labels_}\n    self.epsilon_ = self.var_smoothing * max(self._variance(col) for col in zip(*dataset, strict=False))\n    self.means_, self.variances_ = self._get_classes_means_variances(dataset, labels)\n    return self\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes","title":"toyml.classification.naive_bayes.MultinomialNaiveBayes  <code>dataclass</code>","text":"<pre><code>MultinomialNaiveBayes(class_prior_: dict[Class, float] = dict(), alpha: float = 1.0, labels_: list[Class] = list(), class_count_: int = 0, class_feature_count_: dict[Class, list[int]] = dict(), class_feature_log_prob_: dict[Class, list[float]] = dict())\n</code></pre> <p>               Bases: <code>BaseNaiveBayes</code></p> <p>Multinomial Naive Bayes classifier.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import random\n&gt;&gt;&gt; rng = random.Random(0)\n&gt;&gt;&gt; dataset = [[rng.randint(0, 5) for _ in range(100)] for _ in range(6)]\n&gt;&gt;&gt; label = [1, 2, 3, 4, 5, 6]\n&gt;&gt;&gt; clf = MultinomialNaiveBayes().fit(dataset, label)\n&gt;&gt;&gt; clf.predict(dataset[2])\n3\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.alpha","title":"alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alpha: float = 1.0\n</code></pre> <p>Additive (Laplace/Lidstone) smoothing parameter</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[Class] = field(default_factory=list)\n</code></pre> <p>The labels in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.class_count_","title":"class_count_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_count_: int = 0\n</code></pre> <p>The number of classes in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.class_prior_","title":"class_prior_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_prior_: dict[Class, float] = field(default_factory=dict)\n</code></pre> <p>The prior probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.class_feature_count_","title":"class_feature_count_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_feature_count_: dict[Class, list[int]] = field(default_factory=dict)\n</code></pre> <p>The feature value counts of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.class_feature_log_prob_","title":"class_feature_log_prob_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_feature_log_prob_: dict[Class, list[float]] = field(default_factory=dict)\n</code></pre> <p>The feature value probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.MultinomialNaiveBayes.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; MultinomialNaiveBayes\n</code></pre> <p>Fit the Multinomial Naive Bayes classifier.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>Training data, where each row is a sample and each column is a feature.      Features should be represented as counts (non-negative integers).</p> <p> TYPE: <code>list[list[FeatureValue]]</code> </p> <code>labels</code> <p>Target labels for training data.</p> <p> TYPE: <code>list[Class]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>Returns the instance itself.</p> <p> TYPE: <code>MultinomialNaiveBayes</code> </p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def fit(self, dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; MultinomialNaiveBayes:\n    \"\"\"Fit the Multinomial Naive Bayes classifier.\n\n    Args:\n        dataset: Training data, where each row is a sample and each column is a feature.\n                 Features should be represented as counts (non-negative integers).\n        labels: Target labels for training data.\n\n    Returns:\n        self: Returns the instance itself.\n    \"\"\"\n    self.labels_ = sorted(set(labels))\n    self.class_count_ = len(set(labels))\n    # get the prior from training dataset labels\n    self.class_prior_ = {label: count / len(dataset) for label, count in Counter(labels).items()}\n    self.class_feature_count_, self.class_feature_log_prob_ = self._get_classes_feature_count_prob(dataset, labels)\n    return self\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes","title":"toyml.classification.naive_bayes.CategoricalNaiveBayes  <code>dataclass</code>","text":"<pre><code>CategoricalNaiveBayes(class_prior_: dict[Class, float] = dict(), alpha: float = 1.0, labels_: list[Class] = list(), class_count_: int = 0, class_feature_count_: dict[Class, dict[Dimension, dict[FeatureValue, float]]] = dict(), class_feature_log_prob_: dict[Class, dict[Dimension, dict[FeatureValue, float]]] = dict())\n</code></pre> <p>               Bases: <code>BaseNaiveBayes</code></p> <p>Categorical Naive Bayes classifier.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import random\n&gt;&gt;&gt; rng = random.Random(0)\n&gt;&gt;&gt; dataset = [[rng.randint(0, 5) for _ in range(100)] for _ in range(6)]\n&gt;&gt;&gt; label = [1, 2, 3, 4, 5, 6]\n&gt;&gt;&gt; clf = CategoricalNaiveBayes().fit(dataset, label)\n&gt;&gt;&gt; clf.predict(dataset[2])\n3\n</code></pre>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.alpha","title":"alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alpha: float = 1.0\n</code></pre> <p>Additive (Laplace/Lidstone) smoothing parameter</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[Class] = field(default_factory=list)\n</code></pre> <p>The labels in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.class_count_","title":"class_count_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_count_: int = 0\n</code></pre> <p>The number of classes in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.class_prior_","title":"class_prior_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_prior_: dict[Class, float] = field(default_factory=dict)\n</code></pre> <p>The prior probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.class_feature_count_","title":"class_feature_count_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_feature_count_: dict[Class, dict[Dimension, dict[FeatureValue, float]]] = field(default_factory=dict)\n</code></pre> <p>The feature value counts of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.class_feature_log_prob_","title":"class_feature_log_prob_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>class_feature_log_prob_: dict[Class, dict[Dimension, dict[FeatureValue, float]]] = field(default_factory=dict)\n</code></pre> <p>The feature value probability of each class in training dataset</p>"},{"location":"algorithms/classification/naive_bayes/#toyml.classification.naive_bayes.CategoricalNaiveBayes.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; CategoricalNaiveBayes\n</code></pre> <p>Fit the Categorical Naive Bayes classifier.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>Training data, where each row is a sample and each column is a feature.</p> <p> TYPE: <code>list[list[FeatureValue]]</code> </p> <code>labels</code> <p>Target labels for training data.</p> <p> TYPE: <code>list[Class]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>Returns the instance itself.</p> <p> TYPE: <code>CategoricalNaiveBayes</code> </p> Source code in <code>toyml/classification/naive_bayes.py</code> <pre><code>def fit(self, dataset: list[list[FeatureValue]], labels: list[Class]) -&gt; CategoricalNaiveBayes:\n    \"\"\"Fit the Categorical Naive Bayes classifier.\n\n    Args:\n        dataset: Training data, where each row is a sample and each column is a feature.\n        labels: Target labels for training data.\n\n    Returns:\n        self: Returns the instance itself.\n    \"\"\"\n    self.labels_ = sorted(set(labels))\n    self.class_count_ = len(set(labels))\n    # get the prior from training dataset labels\n    self.class_prior_ = {label: count / len(dataset) for label, count in Counter(labels).items()}\n    self.class_feature_count_, self.class_feature_log_prob_ = self._get_classes_feature_count_prob(dataset, labels)\n    return self\n</code></pre>"},{"location":"algorithms/clustering/agnes/","title":"AGNES","text":""},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES","title":"toyml.clustering.agnes.AGNES  <code>dataclass</code>","text":"<pre><code>AGNES(n_cluster: int, linkage: Literal['single', 'complete', 'average'] = 'single', distance_metric: Literal['euclidean'] = 'euclidean', distance_matrix_: list[list[float]] = list(), clusters_: list[ClusterTree] = list(), labels_: list[int] = list(), cluster_tree_: ClusterTree | None = None, linkage_matrix: list[list[float]] = list(), _cluster_index: int = 0)\n</code></pre> <p>Agglomerative clustering algorithm (Bottom-up Hierarchical Clustering).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import AGNES\n&gt;&gt;&gt; dataset = [[1, 0], [1, 1], [1, 2], [10, 0], [10, 1], [10, 2]]\n&gt;&gt;&gt; agnes = AGNES(n_cluster=2).fit(dataset)\n&gt;&gt;&gt; print(agnes.labels_)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Using fit_predict method\n&gt;&gt;&gt; labels = agnes.fit_predict(dataset)\n&gt;&gt;&gt; print(labels)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Using different linkage methods\n&gt;&gt;&gt; agnes = AGNES(n_cluster=2, linkage=\"complete\").fit(dataset)\n&gt;&gt;&gt; print(agnes.labels_)\n[0, 0, 0, 1, 1, 1]\n</code></pre> <pre><code>&gt;&gt;&gt; # Plotting dendrogram\n&gt;&gt;&gt; agnes = AGNES(n_cluster=1).fit(dataset)\n&gt;&gt;&gt; agnes.plot_dendrogram(show=True)\n</code></pre> The AGNES Dendrogram Plot <p></p> References <ol> <li>Zhou Zhihua</li> <li>Tan</li> </ol>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.n_cluster","title":"n_cluster  <code>instance-attribute</code>","text":"<pre><code>n_cluster: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.linkage","title":"linkage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linkage: Literal['single', 'complete', 'average'] = 'single'\n</code></pre> <p>The linkage method to use.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.distance_metric","title":"distance_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_metric: Literal['euclidean'] = 'euclidean'\n</code></pre> <p>The distance metric to use.(For now we only support euclidean).</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.distance_matrix_","title":"distance_matrix_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_matrix_: list[list[float]] = field(default_factory=list)\n</code></pre> <p>The distance matrix.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.clusters_","title":"clusters_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters_: list[ClusterTree] = field(default_factory=list)\n</code></pre> <p>The clusters.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The labels of each sample.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; AGNES\n</code></pre> <p>Fit the model.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; AGNES:\n    \"\"\"Fit the model.\"\"\"\n    self._validate(dataset)\n    n = len(dataset)\n    self.clusters_ = [ClusterTree(cluster_index=i, sample_indices=[i]) for i in range(n)]\n    self._cluster_index = n\n    self.distance_matrix_ = self._get_init_distance_matrix(dataset)\n    while len(self.clusters_) &gt; self.n_cluster:\n        (i, j), cluster_ij_distance = self._get_closest_clusters()\n        # merge cluster_i and cluster_j\n        self._merge_clusters(i, j, cluster_ij_distance)\n        # update distance matrix\n        self._update_distance_matrix(dataset, i, j)\n    # build cluster_tree_\n    self.cluster_tree_ = self._build_cluster_tree(n)\n    # assign dataset labels\n    self._get_labels(len(dataset))\n    return self\n</code></pre>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit the model and return the labels of each sample.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"Fit the model and return the labels of each sample.\"\"\"\n    self.fit(dataset)\n    return self.labels_\n</code></pre>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.AGNES.plot_dendrogram","title":"plot_dendrogram","text":"<pre><code>plot_dendrogram(figure_name: str = 'agnes_dendrogram.png', show: bool = False) -&gt; None\n</code></pre> <p>Plot the dendrogram of the clustering result.</p> <p>This method visualizes the hierarchical structure of the clustering using a dendrogram. It requires the number of clusters to be set to 1 during initialization.</p> PARAMETER DESCRIPTION <code>figure_name</code> <p>The filename for saving the plot.                Defaults to \"agnes_dendrogram.png\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'agnes_dendrogram.png'</code> </p> <code>show</code> <p>If True, displays the plot. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the number of clusters is not 1.</p> Note <p>This method requires matplotlib and scipy to be installed.</p> Source code in <code>toyml/clustering/agnes.py</code> <pre><code>def plot_dendrogram(\n    self,\n    figure_name: str = \"agnes_dendrogram.png\",\n    show: bool = False,\n) -&gt; None:\n    \"\"\"Plot the dendrogram of the clustering result.\n\n    This method visualizes the hierarchical structure of the clustering\n    using a dendrogram. It requires the number of clusters to be set to 1\n    during initialization.\n\n    Args:\n        figure_name: The filename for saving the plot.\n                           Defaults to \"agnes_dendrogram.png\".\n        show: If True, displays the plot. Defaults to False.\n\n    Raises:\n        ValueError: If the number of clusters is not 1.\n\n    Note:\n        This method requires matplotlib and scipy to be installed.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from scipy.cluster.hierarchy import dendrogram\n\n    if self.n_cluster != 1:\n        msg = \"The number of clusters should be 1 to plot dendrogram\"\n        raise ValueError(msg)\n    # Plot the dendrogram\n    plt.figure(figsize=(10, 7))\n    dendrogram(np.array(self.linkage_matrix))\n    plt.title(\"AGNES Dendrogram\")\n    plt.xlabel(\"Sample Index\")\n    plt.ylabel(\"Distance\")\n    plt.savefig(f\"{figure_name}\", dpi=300, bbox_inches=\"tight\")\n    if show:\n        plt.show()\n</code></pre>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree","title":"toyml.clustering.agnes.ClusterTree  <code>dataclass</code>","text":"<pre><code>ClusterTree(cluster_index: int, parent: ClusterTree | None = None, children: list[ClusterTree] = list(), sample_indices: list[int] = list(), children_cluster_distance: float | None = None)\n</code></pre> <p>Represents a node in the hierarchical clustering tree.</p> <p>Each node is a cluster containing sample indices. Leaf nodes represent individual samples, while internal nodes represent merged clusters. The root node contains all samples.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.parent","title":"parent  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parent: ClusterTree | None = None\n</code></pre> <p>Parent node.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.children","title":"children  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>children: list[ClusterTree] = field(default_factory=list)\n</code></pre> <p>Children nodes.</p>"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.ClusterTree.sample_indices","title":"sample_indices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_indices: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster: dataset sample indices.</p>"},{"location":"algorithms/clustering/dbscan/","title":"DBSCAN","text":""},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN","title":"toyml.clustering.dbscan.DBSCAN  <code>dataclass</code>","text":"<pre><code>DBSCAN(eps: float = 0.5, min_samples: int = 5, clusters_: list[list[int]] = list(), core_objects_: set[int] = set(), noises_: list[int] = list(), labels_: list[int] = list())\n</code></pre> <p>DBSCAN algorithm.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import DBSCAN\n&gt;&gt;&gt; dataset = [[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]\n&gt;&gt;&gt; dbscan = DBSCAN(eps=3, min_samples=2).fit(dataset)\n&gt;&gt;&gt; dbscan.clusters_\n[[0, 1, 2], [3, 4]]\n&gt;&gt;&gt; dbscan.noises_\n[5]\n&gt;&gt;&gt; dbscan.labels_\n[0, 0, 0, 1, 1, -1]\n</code></pre> References <ol> <li>Zhou Zhihua</li> <li>Han</li> <li>Kassambara</li> </ol>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.eps","title":"eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eps: float = 0.5\n</code></pre> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function. (same as sklearn)</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.min_samples","title":"min_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_samples: int = 5\n</code></pre> <p>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. If min_samples is set to a higher value, DBSCAN will find denser clusters, whereas if it is set to a lower value, the found clusters will be more sparse. (same as sklearn)</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.clusters_","title":"clusters_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters_: list[list[int]] = field(default_factory=list)\n</code></pre> <p>The clusters found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.core_objects_","title":"core_objects_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>core_objects_: set[int] = field(default_factory=set)\n</code></pre> <p>The core objects found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.noises_","title":"noises_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>noises_: list[int] = field(default_factory=list)\n</code></pre> <p>The noises found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels found by the DBSCAN algorithm.</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.fit","title":"fit","text":"<pre><code>fit(data: list[list[float]]) -&gt; DBSCAN\n</code></pre> <p>Fit the DBSCAN model.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>self</code> <p>The fitted DBSCAN model.</p> <p> TYPE: <code>DBSCAN</code> </p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def fit(self, data: list[list[float]]) -&gt; DBSCAN:\n    \"\"\"Fit the DBSCAN model.\n\n    Args:\n        data: The dataset.\n\n    Returns:\n        self: The fitted DBSCAN model.\n    \"\"\"\n    dataset = Dataset(data)\n\n    # initialize the unvisited set\n    unvisited = set(range(dataset.n))\n    # get core objects\n    self.core_objects_, self.noises_ = dataset.get_core_objects(self.eps, self.min_samples)\n\n    # core objects used for training\n    if len(self.core_objects_) == 0:\n        logger.warning(\"No core objects found, all data points are noise. Try to adjust the hyperparameters.\")\n        return self\n\n    # set of core objects: unordered\n    core_object_set = self.core_objects_.copy()\n    while core_object_set:\n        unvisited_old = unvisited.copy()\n        core_object = core_object_set.pop()\n        queue: deque[int] = deque()\n        queue.append(core_object)\n        unvisited.remove(core_object)\n\n        while queue:\n            q = queue.popleft()\n            neighbors = dataset.get_neighbors(q, self.eps)\n            if len(neighbors) + 1 &gt;= self.min_samples:\n                delta = set(neighbors) &amp; unvisited\n                for point in delta:\n                    queue.append(point)\n                    unvisited.remove(point)\n\n        cluster = unvisited_old - unvisited\n        self.clusters_.append(list(cluster))\n        core_object_set -= cluster\n\n    self.labels_ = [-1] * dataset.n  # -1 means noise\n    for i, cluster_index in enumerate(self.clusters_):\n        for j in cluster_index:\n            self.labels_[j] = i\n    return self\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DBSCAN.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(data: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit the DBSCAN model and return the cluster labels.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>The cluster labels.</p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def fit_predict(self, data: list[list[float]]) -&gt; list[int]:\n    \"\"\"Fit the DBSCAN model and return the cluster labels.\n\n    Args:\n        data: The dataset.\n\n    Returns:\n        The cluster labels.\n    \"\"\"\n    return self.fit(data).labels_\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset","title":"toyml.clustering.dbscan.Dataset  <code>dataclass</code>","text":"<pre><code>Dataset(data: list[list[float]], distance_metric: Literal['euclidean'] = 'euclidean')\n</code></pre> <p>Dataset for DBSCAN.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> ATTRIBUTE DESCRIPTION <code>data</code> <p>The dataset.</p> <p> TYPE: <code>list[list[float]]</code> </p> <code>n</code> <p>The number of data points.</p> <p> TYPE: <code>int</code> </p> <code>distance_matrix_</code> <p>The distance matrix.</p> <p> TYPE: <code>list[list[float]]</code> </p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset.distance_metric","title":"distance_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_metric: Literal['euclidean'] = 'euclidean'\n</code></pre> <p>The distance metric to use.(For now we only support euclidean).</p>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset.get_neighbors","title":"get_neighbors","text":"<pre><code>get_neighbors(i: int, eps: float) -&gt; list[int]\n</code></pre> <p>Get the neighbors of the i-th data point.</p> PARAMETER DESCRIPTION <code>i</code> <p>The index of the data point.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>The indices of the neighbors (Don't include the point itself).</p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def get_neighbors(self, i: int, eps: float) -&gt; list[int]:\n    \"\"\"Get the neighbors of the i-th data point.\n\n    Args:\n        i: The index of the data point.\n        eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n\n    Returns:\n        The indices of the neighbors (Don't include the point itself).\n    \"\"\"\n    return [j for j in range(self.n) if i != j and self.distance_matrix_[i][j] &lt;= eps]\n</code></pre>"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.Dataset.get_core_objects","title":"get_core_objects","text":"<pre><code>get_core_objects(eps: float, min_samples: int) -&gt; tuple[set[int], list[int]]\n</code></pre> <p>Get the core objects and noises of the dataset.</p> PARAMETER DESCRIPTION <code>eps</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other.</p> <p> TYPE: <code>float</code> </p> <code>min_samples</code> <p>The number of samples (or total weight) in a neighborhood</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>core_objects</code> <p>The indices of the core objects.</p> <p> TYPE: <code>set[int]</code> </p> <code>noises</code> <p>The indices of the noises.</p> <p> TYPE: <code>list[int]</code> </p> Source code in <code>toyml/clustering/dbscan.py</code> <pre><code>def get_core_objects(self, eps: float, min_samples: int) -&gt; tuple[set[int], list[int]]:\n    \"\"\"Get the core objects and noises of the dataset.\n\n    Args:\n        eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n        min_samples: The number of samples (or total weight) in a neighborhood\n        for a point to be considered as a core point.\n\n    Returns:\n        core_objects: The indices of the core objects.\n        noises: The indices of the noises.\n    \"\"\"\n    core_objects = set()\n    noises = []\n    for i in range(self.n):\n        neighbors = self.get_neighbors(i, eps)\n        if len(neighbors) + 1 &gt;= min_samples:  # +1 to include the point itself\n            core_objects.add(i)\n        else:\n            noises.append(i)\n    return core_objects, noises\n</code></pre>"},{"location":"algorithms/clustering/diana/","title":"DIANA: Bisecting Kmeans","text":""},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans","title":"toyml.clustering.bisect_kmeans.BisectingKmeans  <code>dataclass</code>","text":"<pre><code>BisectingKmeans(k: int, cluster_tree_: ClusterTree = ClusterTree(), labels_: list[int] = list())\n</code></pre> <p>Bisecting K-means algorithm.</p> <p>Belong to Divisive hierarchical clustering (DIANA) algorithm.(top-down).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import BisectingKmeans\n&gt;&gt;&gt; dataset = [[1.0, 1.0], [1.0, 2.0], [2.0, 1.0], [10.0, 1.0], [10.0, 2.0], [11.0, 1.0]]\n&gt;&gt;&gt; bisect_kmeans = BisectingKmeans(k=3)\n&gt;&gt;&gt; labels = bisect_kmeans.fit_predict(dataset)\n&gt;&gt;&gt; clusters = bisect_kmeans.cluster_tree_.get_clusters()\n</code></pre> References <ol> <li>Harrington</li> <li>Tan</li> </ol> See Also <ul> <li>K-means algorithm: toyml.clustering.kmeans</li> </ul>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.cluster_tree_","title":"cluster_tree_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cluster_tree_: ClusterTree = field(default_factory=ClusterTree)\n</code></pre> <p>The cluster tree</p>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels of the dataset.</p>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; BisectingKmeans\n</code></pre> <p>Fit the dataset with Bisecting K-means algorithm.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The set of data points for clustering.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>BisectingKmeans</code> <p>self.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; BisectingKmeans:\n    \"\"\"Fit the dataset with Bisecting K-means algorithm.\n\n    Args:\n        dataset: The set of data points for clustering.\n\n    Returns:\n        self.\n\n    \"\"\"\n    n = len(dataset)\n    # check dataset\n    if self.k &gt; n:\n        msg = f\"Number of clusters(k) cannot be greater than the number of samples(n), not get {self.k=} &gt; {n=}\"\n        raise ValueError(\n            msg,\n        )\n    # start with only one cluster which contains all the data points in dataset\n    cluster = list(range(n))\n    self.cluster_tree_.cluster = cluster\n    self.cluster_tree_.centroid = self._get_cluster_centroids(dataset, cluster)\n    self.labels_ = self._predict_dataset_labels(dataset)\n    total_error = sum_square_error([dataset[i] for i in cluster])\n    # iterate until got k clusters\n    while len(self.cluster_tree_.get_clusters()) &lt; self.k:\n        # init values for later iteration\n        to_split_cluster_node = None\n        split_cluster_into: tuple[list[int], list[int]] | None = None\n        for _, cluster_node in enumerate(self.cluster_tree_.leaf_cluster_nodes()):\n            # perform K-means with k=2\n            cluster_data = [dataset[i] for i in cluster_node.cluster]\n            # If the cluster cannot be split further, skip it\n            if len(cluster_data) &lt; 2:\n                continue\n            # Bisect by kmeans with k=2\n            cluster_unsplit_error, cluster_split_error, (cluster1, cluster2) = self._bisect_by_kmeans(\n                cluster_data,\n                cluster_node,\n                dataset,\n            )\n            new_total_error = total_error - cluster_unsplit_error + cluster_split_error\n            if new_total_error &lt; total_error:\n                total_error = new_total_error\n                split_cluster_into = (cluster1, cluster2)\n                to_split_cluster_node = cluster_node\n\n        if to_split_cluster_node is not None and split_cluster_into is not None:\n            self._commit_split(to_split_cluster_node, split_cluster_into, dataset)\n            self.labels_ = self._predict_dataset_labels(dataset)\n        else:\n            break\n\n    return self\n</code></pre>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit and predict the cluster label of the dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The set of data points for clustering.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>Cluster labels of the dataset samples.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"Fit and predict the cluster label of the dataset.\n\n    Args:\n        dataset: The set of data points for clustering.\n\n    Returns:\n        Cluster labels of the dataset samples.\n    \"\"\"\n    return self.fit(dataset).labels_\n</code></pre>"},{"location":"algorithms/clustering/diana/#toyml.clustering.bisect_kmeans.BisectingKmeans.predict","title":"predict","text":"<pre><code>predict(points: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Predict the cluster label of the given points.</p> PARAMETER DESCRIPTION <code>points</code> <p>A list of data points to predict.</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>A list of predicted cluster labels for the input points.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the model has not been fitted yet.</p> Source code in <code>toyml/clustering/bisect_kmeans.py</code> <pre><code>def predict(self, points: list[list[float]]) -&gt; list[int]:\n    \"\"\"Predict the cluster label of the given points.\n\n    Args:\n        points: A list of data points to predict.\n\n    Returns:\n        A list of predicted cluster labels for the input points.\n\n    Raises:\n        ValueError: If the model has not been fitted yet.\n    \"\"\"\n    if self.cluster_tree_.centroid is None:\n        msg = \"The model has not been fitted yet.\"\n        raise ValueError(msg)\n\n    clusters = self.cluster_tree_.get_clusters()\n    predictions = []\n    for point in points:\n        node = self.cluster_tree_\n        while not node.is_leaf():\n            if node.left is None or node.right is None:\n                msg = \"Invalid cluster tree structure.\"\n                raise ValueError(msg)\n\n            dist_left = euclidean_distance(point, node.left.centroid)  # type: ignore[arg-type]\n            dist_right = euclidean_distance(point, node.right.centroid)  # type: ignore[arg-type]\n\n            node = node.left if dist_left &lt; dist_right else node.right\n        cluster_index = clusters.index(node.cluster)\n        predictions.append(cluster_index)\n\n    return predictions\n</code></pre>"},{"location":"algorithms/clustering/kmeans/","title":"Kmeans","text":""},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans","title":"toyml.clustering.kmeans.Kmeans  <code>dataclass</code>","text":"<pre><code>Kmeans(k: int, max_iter: int = 500, tol: float = 1e-05, centroids_init_method: Literal['random', 'kmeans++'] = 'random', random_seed: int | None = None, distance_metric: Literal['euclidean'] = 'euclidean', iter_: int = 0, clusters_: dict[int, list[int]] = dict(), centroids_: dict[int, list[float]] = dict(), labels_: list[int] = list())\n</code></pre> <p>K-means algorithm (with Kmeans++ initialization as option).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import Kmeans\n&gt;&gt;&gt; dataset = [[1.0, 2.0], [1.0, 4.0], [1.0, 0.0], [10.0, 2.0], [10.0, 4.0], [11.0, 0.0]]\n&gt;&gt;&gt; kmeans = Kmeans(k=2, random_seed=42).fit(dataset)\n&gt;&gt;&gt; kmeans.clusters_\n{0: [3, 4, 5], 1: [0, 1, 2]}\n&gt;&gt;&gt; kmeans.centroids_\n{0: [10.333333333333334, 2.0], 1: [1.0, 2.0]}\n&gt;&gt;&gt; kmeans.labels_\n[1, 1, 1, 0, 0, 0]\n&gt;&gt;&gt; kmeans.predict([0, 1])\n1\n&gt;&gt;&gt; kmeans.iter_\n2\n</code></pre> <p>There is a <code>fit_predict</code> method that can be used to fit and predict.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.clustering import Kmeans\n&gt;&gt;&gt; dataset = [[1, 0], [1, 1], [1, 2], [10, 0], [10, 1], [10, 2]]\n&gt;&gt;&gt; Kmeans(k=2, random_seed=42).fit_predict(dataset)\n[1, 1, 1, 0, 0, 0]\n</code></pre> References <ol> <li>Zhou Zhihua</li> <li>Murphy</li> </ol> Note <p>Here we just implement the naive K-means algorithm.</p> See Also <ul> <li>Bisecting K-means algorithm: toyml.clustering.bisect_kmeans</li> </ul>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k: int\n</code></pre> <p>The number of clusters, specified by user.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.max_iter","title":"max_iter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_iter: int = 500\n</code></pre> <p>The number of iterations the algorithm will run for if it does not converge before that.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.tol","title":"tol  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tol: float = 1e-05\n</code></pre> <p>The tolerance for convergence.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.centroids_init_method","title":"centroids_init_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>centroids_init_method: Literal['random', 'kmeans++'] = 'random'\n</code></pre> <p>The method to initialize the centroids.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int | None = None\n</code></pre> <p>The random seed used to initialize the centroids.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.distance_metric","title":"distance_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>distance_metric: Literal['euclidean'] = 'euclidean'\n</code></pre> <p>The distance metric to use.(For now we only support euclidean).</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.clusters_","title":"clusters_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clusters_: dict[int, list[int]] = field(default_factory=dict)\n</code></pre> <p>The clusters of the dataset.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.centroids_","title":"centroids_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>centroids_: dict[int, list[float]] = field(default_factory=dict)\n</code></pre> <p>The centroids of the clusters.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.labels_","title":"labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>labels_: list[int] = field(default_factory=list)\n</code></pre> <p>The cluster labels of the dataset.</p>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; Kmeans\n</code></pre> <p>Fit the dataset with K-means algorithm.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the set of data points for clustering</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>Kmeans</code> <p>self.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; Kmeans:\n    \"\"\"Fit the dataset with K-means algorithm.\n\n    Args:\n        dataset: the set of data points for clustering\n\n    Returns:\n        self.\n    \"\"\"\n    self.centroids_ = self._get_initial_centroids(dataset)\n    for _ in range(self.max_iter):\n        self.iter_ += 1\n        prev_centroids = self.centroids_\n        self._iter_step(dataset)\n        if self._is_converged(prev_centroids):\n            break\n    return self\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(dataset: list[list[float]]) -&gt; list[int]\n</code></pre> <p>Fit and predict the cluster label of the dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>the set of data points for clustering</p> <p> TYPE: <code>list[list[float]]</code> </p> RETURNS DESCRIPTION <code>list[int]</code> <p>Cluster labels of the dataset samples.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def fit_predict(self, dataset: list[list[float]]) -&gt; list[int]:\n    \"\"\"Fit and predict the cluster label of the dataset.\n\n    Args:\n        dataset: the set of data points for clustering\n\n    Returns:\n        Cluster labels of the dataset samples.\n    \"\"\"\n    return self.fit(dataset).labels_\n</code></pre>"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.Kmeans.predict","title":"predict","text":"<pre><code>predict(point: list[float]) -&gt; int\n</code></pre> <p>Predict the label of the point.</p> PARAMETER DESCRIPTION <code>point</code> <p>The data point to predict.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The label of the point.</p> Source code in <code>toyml/clustering/kmeans.py</code> <pre><code>def predict(self, point: list[float]) -&gt; int:\n    \"\"\"Predict the label of the point.\n\n    Args:\n        point: The data point to predict.\n\n    Returns:\n        The label of the point.\n\n    \"\"\"\n    if len(self.centroids_) == 0:\n        msg = \"The model is not fitted yet\"\n        raise ValueError(msg)\n    return self._get_point_centroid_label(point, self.centroids_)\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/","title":"AdaBoost","text":""},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost","title":"toyml.ensemble.adaboost.AdaBoost  <code>dataclass</code>","text":"<pre><code>AdaBoost(weak_learner: type[BaseWeakLeaner], n_weak_learner: int = 5, predict_labels_: list[int] | None = None, training_error_rate_: float | None = None, _n: int = -1, _labels: list[int] = list(), _weights: list[float] = list(), _base_clf_labels: list[list[int]] = list(), _weak_learner_predicts: list[Callable[..., Any]] = list(), _alphas: list[float] = list())\n</code></pre> <p>The implementation of AdaBoost algorithm.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.ensemble.adaboost import AdaBoost, OneDimensionClassifier\n&gt;&gt;&gt; dataset = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n&gt;&gt;&gt; labels = [1, 1, 1, -1, -1, -1, 1, 1, 1, -1]\n&gt;&gt;&gt; ada = AdaBoost(weak_learner=OneDimensionClassifier, n_weak_learner=3).fit(dataset, labels)\n&gt;&gt;&gt; print(f\"Training dataset error rate: {ada.training_error_rate_}\")\nTraining dataset error rate: 0.0\n&gt;&gt;&gt; test_sample = [1.5]\n&gt;&gt;&gt; print(f\"The label of {test_sample} is {ada.predict(test_sample)}\")\nThe label of [1.5] is 1\n</code></pre> References <ol> <li>Li Hang</li> <li>Zhou Zhihua</li> </ol>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.weak_learner","title":"weak_learner  <code>instance-attribute</code>","text":"<pre><code>weak_learner: type[BaseWeakLeaner]\n</code></pre> <p>The weak learner to be used in the AdaBoost algorithm.</p>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.n_weak_learner","title":"n_weak_learner  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_weak_learner: int = 5\n</code></pre> <p>The number of weak learners to be used in the AdaBoost algorithm.</p>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.predict_labels_","title":"predict_labels_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>predict_labels_: list[int] | None = None\n</code></pre> <p>The prediction labels of the training dataset.</p>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.training_error_rate_","title":"training_error_rate_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>training_error_rate_: float | None = None\n</code></pre> <p>The error rate of the training dataset.</p>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]], labels: list[int]) -&gt; AdaBoost\n</code></pre> <p>Fit the AdaBoost model.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def fit(\n    self,\n    dataset: list[list[float]],\n    labels: list[int],\n) -&gt; AdaBoost:\n    \"\"\"Fit the AdaBoost model.\"\"\"\n    self._labels = labels\n    # for model training(Gm)\n    self._n = len(labels)\n    self._weights: Any = [1.0 / self._n] * self._n\n    # we use -2 to initialize the class which can handle cases\n    # such as multi-classes(0, 1, 2, ...) and binary classes(-1, 1)\n    self._base_clf_labels = [[-2] * self._n for _ in range(self.n_weak_learner)]\n    # base clf models\n    self._weak_learner_predicts: list[Callable[..., int]] = []\n    self._alphas = [0.0] * self.n_weak_learner\n\n    for m in range(self.n_weak_learner):\n        model = self.weak_learner().fit(dataset, self._weights, self._labels)\n        self._base_clf_labels[m] = model.get_predict_labels()\n        self._weak_learner_predicts.append(model.predict)\n        error_rate = model.get_error_rate()\n        # Warning when the error rate is too large\n        if error_rate &gt; 0.5:\n            logger.warning(f\"Weak learner error rate = {error_rate} &lt; 0.5\")  # noqa: G004\n        alpha = 0.5 * math.log((1 - error_rate) / error_rate)\n        self._alphas[m] = alpha\n        # update the weights\n        weights = [0.0] * self._n\n        for i in range(self._n):\n            weights[i] = self._weights[i] * math.exp(-alpha * self._labels[i] * self._base_clf_labels[m][i])\n        self._weights = [weight / sum(weights) for weight in weights]\n    # collect training dataset result\n    self.predict_labels_ = [self.predict(x) for x in dataset]\n    self.training_error_rate_ = sum(self.predict_labels_[i] != self._labels[i] for i in range(self._n)) / self._n\n    return self\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.AdaBoost.predict","title":"predict","text":"<pre><code>predict(x: list[float]) -&gt; int\n</code></pre> <p>Predict the label of the input sample.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def predict(self, x: list[float]) -&gt; int:\n    \"\"\"Predict the label of the input sample.\"\"\"\n    ensemble_predict = 0\n    for m in range(self.n_weak_learner):\n        model_predict = self._weak_learner_predicts[m]\n        ensemble_predict += self._alphas[m] * model_predict(x)\n    if ensemble_predict &gt;= 0:\n        return 1\n    return -1\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier","title":"toyml.ensemble.adaboost.OneDimensionClassifier  <code>dataclass</code>","text":"<pre><code>OneDimensionClassifier(_sign_mode: SignMode = POS_NEG, _best_cut: float = inf, error_rate_: float = inf, predict_labels_: list[int] | None = None)\n</code></pre> <p>               Bases: <code>BaseWeakLeaner</code></p> <p>Binary classifier with one dimension feature.</p> <p>Ref: Li Hang, 1 ed, E8.1.3</p>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]], weights: list[float], labels: list[int]) -&gt; OneDimensionClassifier\n</code></pre> <p>Fit the one-dimension classifier.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def fit(\n    self,\n    dataset: list[list[float]],\n    weights: list[float],\n    labels: list[int],\n) -&gt; OneDimensionClassifier:\n    \"\"\"Fit the one-dimension classifier.\"\"\"\n    # search for the best cut point\n    sign_mode, best_cut, best_error_rate = self.get_best_cut(dataset, weights, labels)\n    self.error_rate_ = best_error_rate\n    self._best_cut = best_cut\n    self._sign_mode = sign_mode\n    # get labels\n    self.predict_labels_ = [0] * len(labels)\n    for i, x in enumerate(dataset):\n        self.predict_labels_[i] = self.predict(x)\n    return self\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier.predict","title":"predict","text":"<pre><code>predict(x: list[float]) -&gt; int\n</code></pre> <p>Predict the label of the input sample.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def predict(self, x: list[float]) -&gt; int:\n    \"\"\"Predict the label of the input sample.\"\"\"\n    if self._best_cut is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n    if self._sign_mode == \"POS_NEG\":\n        if x[0] &lt;= self._best_cut:\n            return 1\n        return -1\n    if x[0] &lt;= self._best_cut:\n        return -1\n    return 1\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier.get_error_rate","title":"get_error_rate","text":"<pre><code>get_error_rate() -&gt; float\n</code></pre> <p>Get the error rate of the training dataset.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def get_error_rate(self) -&gt; float:\n    \"\"\"Get the error rate of the training dataset.\"\"\"\n    if self.error_rate_ is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n    return self.error_rate_\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier.get_predict_labels","title":"get_predict_labels","text":"<pre><code>get_predict_labels() -&gt; list[int]\n</code></pre> <p>Get the prediction labels of the training dataset.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def get_predict_labels(self) -&gt; list[int]:\n    \"\"\"Get the prediction labels of the training dataset.\"\"\"\n    if self.predict_labels_ is None:\n        msg = \"The model is not fitted yet!\"\n        raise ValueError(msg)\n    return self.predict_labels_\n</code></pre>"},{"location":"algorithms/ensemble/adaboost/#toyml.ensemble.adaboost.OneDimensionClassifier.get_best_cut","title":"get_best_cut","text":"<pre><code>get_best_cut(dataset: list[list[float]], weights: list[float], labels: list[int]) -&gt; tuple[SignMode, float, float]\n</code></pre> <p>Get the best cut of the training dataset.</p> Source code in <code>toyml/ensemble/adaboost.py</code> <pre><code>def get_best_cut(\n    self,\n    dataset: list[list[float]],\n    weights: list[float],\n    labels: list[int],\n) -&gt; tuple[SignMode, float, float]:\n    \"\"\"Get the best cut of the training dataset.\"\"\"\n    points = [x[0] for x in dataset]\n    candidate_cuts = self._get_candidate_cuts(points)\n    # (func_mode, cut, error_rate)\n    candidate_cuts_result = []\n    for cut in candidate_cuts:\n        pos_neg_error_rate = self._get_cut_error_rate(cut, points, weights, labels, self.SignMode.POS_NEG)\n        neg_pos_error_rate = self._get_cut_error_rate(cut, points, weights, labels, self.SignMode.NEG_POS)\n        candidate_cuts_result.extend(\n            [(self.SignMode.POS_NEG, cut, pos_neg_error_rate), (self.SignMode.NEG_POS, cut, neg_pos_error_rate)],\n        )\n\n    # sorted by error rate\n    best_cut_result = sorted(candidate_cuts_result, key=lambda x: x[2])[0]\n    sign_mode, best_cut, best_error_rate = best_cut_result\n    return sign_mode, best_cut, best_error_rate\n</code></pre>"},{"location":"algorithms/ensemble/iforest/","title":"Isolation Forest","text":""},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest","title":"toyml.ensemble.iforest.IsolationForest  <code>dataclass</code>","text":"<pre><code>IsolationForest(n_itree: int = 100, max_samples: None | int = None, score_threshold: float = 0.5, random_seed: int | None = None, itrees_: list[IsolationTree] = list())\n</code></pre> <p>Isolation Forest.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from toyml.ensemble.iforest import IsolationForest\n&gt;&gt;&gt; dataset = [[-1.1], [0.3], [0.5], [100.0]]\n&gt;&gt;&gt; IsolationForest(n_itree=100, max_samples=4).fit_predict(dataset)\n[1, 1, 1, -1]\n</code></pre> References <p>Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. \"Isolation forest.\" 2008 eighth ieee international conference on data mining. IEEE, 2008.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.n_itree","title":"n_itree  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_itree: int = 100\n</code></pre> <p>The number of isolation tree in the ensemble.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.max_samples","title":"max_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_samples: None | int = None\n</code></pre> <p>The number of samples to draw from X to train each base estimator.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.score_threshold","title":"score_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>score_threshold: float = 0.5\n</code></pre> <p>The score threshold that is used to define outlier: If sample's anomaly score &gt; score_threshold, then the sample is detected as outlier(predict return -1); otherwise, the sample is normal(predict return 1)</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int | None = None\n</code></pre> <p>The random seed used to initialize the centroids.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.itrees_","title":"itrees_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>itrees_: list[IsolationTree] = field(default_factory=list)\n</code></pre> <p>The isolation trees in the forest.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.fit","title":"fit","text":"<pre><code>fit(dataset: list[list[float]]) -&gt; IsolationForest\n</code></pre> <p>Fit the isolation forest model.</p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def fit(self, dataset: list[list[float]]) -&gt; IsolationForest:\n    \"\"\"Fit the isolation forest model.\"\"\"\n    if self.max_samples is None or self.max_samples &gt; len(dataset):\n        self.max_samples = len(dataset)\n\n    self.itrees_ = self._fit_itrees(dataset)\n    return self\n</code></pre>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.score","title":"score","text":"<pre><code>score(sample: list[float]) -&gt; float\n</code></pre> <p>Predict the sample's anomaly score.</p> PARAMETER DESCRIPTION <code>sample</code> <p>The data sample.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The anomaly score.</p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def score(self, sample: list[float]) -&gt; float:\n    \"\"\"Predict the sample's anomaly score.\n\n    Args:\n        sample: The data sample.\n\n    Returns:\n        The anomaly score.\n    \"\"\"\n    assert len(self.itrees_) == self.n_itree, \"Please fit the model before score sample!\"\n    assert self.max_samples is not None, \"Please fit the model before score sample!\"\n    itree_path_lengths = [itree.get_sample_path_length(sample) for itree in self.itrees_]\n    expect_path_length = sum(itree_path_lengths) / len(itree_path_lengths)\n    score = 2 ** (-expect_path_length / bst_expect_length(self.max_samples))\n    return score\n</code></pre>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationForest.predict","title":"predict","text":"<pre><code>predict(sample: list[float]) -&gt; int\n</code></pre> <p>Predict the sample is outlier ot not.</p> PARAMETER DESCRIPTION <code>sample</code> <p>The data sample.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>Outlier</code> <p>-1; Normal: 1.</p> <p> TYPE: <code>int</code> </p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def predict(self, sample: list[float]) -&gt; int:\n    \"\"\"Predict the sample is outlier ot not.\n\n    Args:\n        sample: The data sample.\n\n    Returns:\n        Outlier: -1; Normal: 1.\n    \"\"\"\n    score = self.score(sample)\n    # outlier\n    if score &gt; self.score_threshold:\n        return -1\n    return 1\n</code></pre>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree","title":"toyml.ensemble.iforest.IsolationTree  <code>dataclass</code>","text":"<pre><code>IsolationTree(max_height: int, random_seed: int | None = None, sample_size_: int | None = None, feature_num_: int | None = None, left_: IsolationTree | None = None, right_: IsolationTree | None = None, split_at_: int | None = None, split_value_: float | None = None)\n</code></pre> <p>The isolation tree.</p> Note <p>The isolation tree is a proper(full) binary tree, which has either 0 or 2 children.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.max_height","title":"max_height  <code>instance-attribute</code>","text":"<pre><code>max_height: int\n</code></pre> <p>The maximum height of the tree.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int | None = None\n</code></pre> <p>The random seed used to initialize the centroids.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.sample_size_","title":"sample_size_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_size_: int | None = None\n</code></pre> <p>The sample size.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.feature_num_","title":"feature_num_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_num_: int | None = None\n</code></pre> <p>The number of features at each sample.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.left_","title":"left_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>left_: IsolationTree | None = None\n</code></pre> <p>The left child of the tree.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.right_","title":"right_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>right_: IsolationTree | None = None\n</code></pre> <p>The right child of the tree.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.split_at_","title":"split_at_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>split_at_: int | None = None\n</code></pre> <p>The index of feature which is used to split the tree's samples into children.</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.split_value_","title":"split_value_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>split_value_: float | None = None\n</code></pre> <p>The value of split_at feature that use to split samples</p>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.fit","title":"fit","text":"<pre><code>fit(samples: list[list[float]]) -&gt; IsolationTree\n</code></pre> <p>Fit the isolation tree.</p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def fit(self, samples: list[list[float]]) -&gt; IsolationTree:\n    \"\"\"Fit the isolation tree.\"\"\"\n    self.sample_size_ = len(samples)\n    self.feature_num_ = len(samples[0])\n    # exNode\n    if self.max_height == 0 or self.sample_size_ == 1:\n        return self\n    # inNode\n    left_itree, right_itree = self._get_left_right_child_itree(samples)\n    self.left_, self.right_ = left_itree, right_itree\n    return self\n</code></pre>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.get_sample_path_length","title":"get_sample_path_length","text":"<pre><code>get_sample_path_length(sample: list[float]) -&gt; float\n</code></pre> <p>Get the sample's path length to the external(leaf) node.</p> PARAMETER DESCRIPTION <code>sample</code> <p>The data sample.</p> <p> TYPE: <code>list[float]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The path length of the sample.</p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def get_sample_path_length(self, sample: list[float]) -&gt; float:\n    \"\"\"Get the sample's path length to the external(leaf) node.\n\n    Args:\n        sample: The data sample.\n\n    Returns:\n        The path length of the sample.\n    \"\"\"\n    if self.is_external_node():\n        assert self.sample_size_ is not None\n        if self.sample_size_ == 1:\n            return 0\n        return bst_expect_length(self.sample_size_)\n\n    assert self.split_at_ is not None\n    assert self.split_value_ is not None\n    if sample[self.split_at_] &lt; self.split_value_:\n        assert self.left_ is not None\n        return 1 + self.left_.get_sample_path_length(sample)\n    assert self.right_ is not None\n    return 1 + self.right_.get_sample_path_length(sample)\n</code></pre>"},{"location":"algorithms/ensemble/iforest/#toyml.ensemble.iforest.IsolationTree.is_external_node","title":"is_external_node","text":"<pre><code>is_external_node() -&gt; bool\n</code></pre> <p>The tree node is external(leaf) node or not.</p> Source code in <code>toyml/ensemble/iforest.py</code> <pre><code>def is_external_node(self) -&gt; bool:\n    \"\"\"The tree node is external(leaf) node or not.\"\"\"\n    return bool(self.left_ is None and self.right_ is None)\n</code></pre>"}]}